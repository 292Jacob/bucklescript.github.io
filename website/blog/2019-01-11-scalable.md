---
title: In search of lightning feedback loop in a large codebase
---

As software developers, when we touch a codebase, we want the edit-compile cycle, to be as short as possible. Studies show that feedback loop less than 1s, or 100ms will prevent devleopers getting distracted. 

This seems to be possible in a small project, but as project grows to 10k files, or even more, 
100k files in a large company like Google, Facebook, this seems to be extremely challenging.

In this article, we are talking about how BuckleScript is trying to solve this issue.

The edit-build cycle in BuckleScript consists of two components: the compiler which rebuilds
and the scheduler which figures out what to rebuild.

## The importance of compiler's cold performance 

In order to reduce the edit-build latency, some languages adopt an approach of in memory compiler + watch mode. We think this is not a scalable or reliable approach. 

A compiler is a complex piece of software, the chance that a compiler has memory leak is not low. It is not observed in real world since compiler is used mostly in a short-lived setting, it starts up fast and dies off quickly. 

However, this is decimated when compiler is put in server mode. When we have 10k or 100k files held in memory, it is very easy to observe OOM (out of memory issues). 

We figured that to deliver a scalable and reliable system, it is better to decouple the compiler's complexity from the scheduler. When the compiler cold starts and dies off quickly, the operating system process mechanism serves as an obviously correct garbage collector, which increases the reliability of the whole system.

To reduce such latency in a single compiler's workload, we spent lots of time tweaking the performance of the compiler itself, for example, rewriting the hot path in C code, most OCaml code is written in C-style to avoid allocation.

To have a general idea of how fast BuckleScript's compiler runs:

```
test>cat fib.ml
let rec fib = function
  | 0 | 1 -> 1
  | n -> fib (n - 1) + fib (n - 2)
```  

```
test>time /usr/local/lib/node_modules/bs-platform/lib/bsc.exe -bs-cmi -bs-cmj -c fib.ml

real    0m0.008s
user    0m0.004s
sys     0m0.003s
```

Having a compiler run fast with a cold start lays the groundwork so that it won't be the bottleneck in the whole process.

## The art of being incremental

Having a compiler running fast in cold mode does not solve the scalability issue alone. Take a code base which contains 100K source files for example, 100ms per file would result in 10,000s latency which is not acceptable. To solve such issue, we need reduce the workload as much as possible during each edit-build cycle.

Suppose we have two compilation units A , B, and B depends on A. Whenever A changes, B gets recompiled, to make it worse, the recompilation propogates, all dependencies of B get recompiled, which will result in a snowball effect. In this model, whenever we touch the non-leaf compilation unit, the latency would be large. 

The key observation here is that B does not really 
depend on the last modified time of A, it depends on the intermediate output of A (e.g, `.cmj` file) which may not change even if we modify A. What we work on here is to reduce the probability of changing A's intermediate output due to changing A. With the integration of a scheduler, it can help stop the propagation as early as possible.

In BuckleScript, each compilation unit is composed of two files, the implementation file and interface file which are compiled to intermediate output .cmj and .cmi separately. 

The interface builds are completely separate from implemetation, it does not depend on .cmj (implementation intermediate output) at all. 

Suppose the interface is not changed, whenever we touch the implementation file, BuckleScript designs the data structure of .cmj in a way that the content of .cmj is seldom changed. (Let's say its probability is 0.05 which is rare in cases when the arity of a function changed)

If neither .cmj nor .cmi  gets changed, the scheduler would stop propagation.

Suppose the `.cmj` file is still changed (P = 0.05), A's dependency B would get recompiled. Note B.cmi only depends on A.cmi, so it will not get compiled, only B.cmj will get recompiled, its chances that B.cmj gets changed will be even lower. In practice, the probabiity of compiling two compilation units is less than 

```
0.05 * 0.05 = 0.0025
```

This means when we are in an edit-build cycle, whenever A gets changed, it may have many direct dependencies, but the longest rebuild sequence will get settled in at most two compilation units. Suppose the scheduler schedule the tasks in parallel, the longest sequence is bound by two, which means bounded rebuild cycle.

In implementation, we also generalized the idea of stopping propogation of .cmi changes, so whenever we are adding some comments in the interface file, it will 
get settled quickly.

The worst case is that the root of an interface changed which means all its dependencies will get recompiled.


## A fast scheduler

As we said, the time spent in an edit-build cycle is mostly composed of two parts: compiling invoking the compiler, scheduling.

We are reusing the very fast scheduler provided by [Ninja](https://ninja-build.org/manual.html)


> Where other build systems are high-level languages, Ninja aims to be an assembler.


BuckleScript outputs assembler style instructions consumed by 
Ninja which does scheduling very fast and provides good parallelism. By integrating with Ninja's `restat` attribute, we are able to implement the idea of content based build system.

It is fast enough for 99% of use cases. For a project less than 10k files, the time spent in an edit-build cycle is dominated by the compiler, but as project grows to 100K files or even more, the time spent in compiler is stable and bounded due to our incremental design, however, the time will dominated by the scheduler.

For a nop build around 10k files, it takes around 200ms for Ninja to figure out nothing needs to be rebuilt.

The current Ninja model is simple, every time it is invoked, it will re-read build.ninja instructions, check stats of artifacts and do the scheduling.

Instead of making a long-lived compiler, we propose to have a long-lived scheduler. The complexity of a scheduler is significantly lower than a compiler.
Having an in-memory scheduler will help reducing redundant work such as parsing build.ninja instructions which is around of size 2M for 10k files. With the integration of  watch mode, it does not need stat all artifacts each time, this should help increase the scalability of scheduler to 100K files or even more.


To conclude, BuckleScript scales to projects with around 10k files, and with some work in improving the scheduler, we believe it is not too difficult to scale it to 100K files or even more. To reach such enormous scalability in a reliable way, we propose to have a long-lived scheduler and fast cold compiler. The language interface design and the design of the data structure for the implementation will make the longest rebuild sequence bounded by two in most cases. 